<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    
    <title>Theory of Machines  | Collaborative Filtering with tensorflow</title>
    <meta name="HandheldFriendly" content="True">
    <meta name="MobileOptimized" content="320">

    <meta name="viewport" content="width=device-width,minimum-scale=1">
    <meta name="generator" content="Hugo 0.31" />
    
    
      <META NAME="ROBOTS" CONTENT="NOINDEX, NOFOLLOW">
    

    <link href='https://ozkansafak.github.io/dist/main.css' rel='stylesheet' type="text/css" />
    
      
    

    

    <meta property="og:title" content="Collaborative Filtering with tensorflow" />
<meta property="og:description" content="PREDICTION OF MOVIE RATINGS 1. Problem Description We are given a rating matrix $R$ where only some of the entries $R_{ij}$ are provided; otherwise rest of them are missing. The task is to predict the missing entries. As in most Machine Learning problems the assumption here is that there&rsquo;s an underlying stationary pattern as to how users rate the movies.
By the nature of the problem, $R$ is a sparse matrix, where the sparsity comes not from zero entries but from empty records." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://ozkansafak.github.io/posts/collaborative-filtering-tensorflow/" />



<meta property="article:published_time" content="2017-11-22T15:49:35-08:00"/>

<meta property="article:modified_time" content="2017-11-22T15:49:35-08:00"/>











<meta itemprop="name" content="Collaborative Filtering with tensorflow">
<meta itemprop="description" content="PREDICTION OF MOVIE RATINGS 1. Problem Description We are given a rating matrix $R$ where only some of the entries $R_{ij}$ are provided; otherwise rest of them are missing. The task is to predict the missing entries. As in most Machine Learning problems the assumption here is that there&rsquo;s an underlying stationary pattern as to how users rate the movies.
By the nature of the problem, $R$ is a sparse matrix, where the sparsity comes not from zero entries but from empty records.">


<meta itemprop="datePublished" content="2017-11-22T15:49:35-08:00" />
<meta itemprop="dateModified" content="2017-11-22T15:49:35-08:00" />
<meta itemprop="wordCount" content="1562">



<meta itemprop="keywords" content="" />
<meta name="twitter:card" content="summary"/><meta name="twitter:title" content="Collaborative Filtering with tensorflow"/>
<meta name="twitter:description" content="PREDICTION OF MOVIE RATINGS 1. Problem Description We are given a rating matrix $R$ where only some of the entries $R_{ij}$ are provided; otherwise rest of them are missing. The task is to predict the missing entries. As in most Machine Learning problems the assumption here is that there&rsquo;s an underlying stationary pattern as to how users rate the movies.
By the nature of the problem, $R$ is a sparse matrix, where the sparsity comes not from zero entries but from empty records."/>

  </head>

  <body class="ma0 avenir bg-near-white">

    

  <header>
     <script type="text/x-mathjax-config">MathJax.Hub.Config({ tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]} }); 
     </script> 
     <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"> 
     </script> 
    <div class="bg-black">
      <nav class="pv3 ph3 ph4-ns" role="navigation">
  <div class="flex-l justify-between items-center center">
    <a href="https://ozkansafak.github.io/" class="f3 fw2 hover-white no-underline white-90 dib">
      Theory of Machines
    </a>
    <div class="flex-l items-center">
      
      








    </div>
  </div>
</nav>

    </div>
  </header>


    <main class="pb7" role="main">
      
  <div class="flex-l mt2 mw8 center">
    <article class="center cf pv5 ph3 ph4-ns mw7">
      <header>
        <p class="f6 b helvetica tracked">
          POSTS
        </p>
        <h1 class="f1">
          Collaborative Filtering with tensorflow
        </h1>
      </header>
      <div class="nested-copy-line-height lh-copy f4 nested-links nested-img mid-gray">
        

<h1 id="prediction-of-movie-ratings">PREDICTION OF MOVIE RATINGS</h1>

<hr />

<h2 id="1-problem-description">1. Problem Description</h2>

<p>We are given a rating matrix $R$ where only some of the entries $R_{ij}$ are provided; otherwise rest of them are missing. The task is to predict the missing entries. As in most Machine Learning problems the assumption here is that there&rsquo;s an underlying stationary pattern as to how users rate the movies.</p>

<p>By the nature of the problem, $R$ is a sparse matrix, where the sparsity comes not from zero entries but from empty records. Therefor, we represent the training data in 3 columns: $i$: user ID , $j$: movie ID and $R_{ij}$: the rating .</p>

<table>
<thead>
<tr>
<th align="center">$i$</th>
<th align="center">$j$</th>
<th align="center">$R_{ij}$</th>
</tr>
</thead>

<tbody>
<tr>
<td align="center">0</td>
<td align="center">14</td>
<td align="center">3.5</td>
</tr>

<tr>
<td align="center">0</td>
<td align="center">7305</td>
<td align="center">4.0</td>
</tr>

<tr>
<td align="center">0</td>
<td align="center">16336</td>
<td align="center">3.5</td>
</tr>

<tr>
<td align="center">.</td>
<td align="center">.</td>
<td align="center">.</td>
</tr>

<tr>
<td align="center">.</td>
<td align="center">.</td>
<td align="center">.</td>
</tr>

<tr>
<td align="center">1</td>
<td align="center">52</td>
<td align="center">4.0</td>
</tr>

<tr>
<td align="center">1</td>
<td align="center">986</td>
<td align="center">4.0</td>
</tr>

<tr>
<td align="center">1</td>
<td align="center">1455</td>
<td align="center">3.5</td>
</tr>

<tr>
<td align="center">1</td>
<td align="center">1705</td>
<td align="center">5.0</td>
</tr>

<tr>
<td align="center">1</td>
<td align="center">5598</td>
<td align="center">4.0</td>
</tr>

<tr>
<td align="center">.</td>
<td align="center">.</td>
<td align="center">.</td>
</tr>

<tr>
<td align="center">.</td>
<td align="center">.</td>
<td align="center">.</td>
</tr>

<tr>
<td align="center">138493</td>
<td align="center">27278</td>
<td align="center">5.0</td>
</tr>
</tbody>
</table>

<hr />

<h2 id="2-collaborative-filtering-model">2. Collaborative Filtering Model</h2>

<p>The terms <em>Collaborative Filtering</em>, <em>Matrix Factorization</em> and <em>Low-Rank Matrix Factorization</em> all refer to the same recommender system model. In essence, this model is based on the assumption that users who liked the same movies are likely to feel similarly towards other movies. The term <em>collaborative</em> refers to the observation that when a large set of users are involved in rating the movies, these users are effectively collaborating to get better movie ratings for everyone because every new rating will help the algorithm learn better features for the <em>users-movies</em> system. Later, these features are used by the model to make better rating predictions for everyone else.</p>

<p>The Collaborative Filtering Model can also be described as reconstructing a <strong>low rank approximation</strong> of matrix $R$ via its <strong>Singular Value Decomposition</strong> $R = U \cdot \Sigma \cdot V^T$. The low-rank reconstruction is achieved by only retaining the largest $k$ singular values, $R_k=U \cdot \Sigma_k \cdot V^T$.</p>

<p><strong>Eckart-Young Theorem</strong> states that if $R_k$ is the best rank-$k$ approximation of $R$, then it&rsquo;s necessary that</p>

<ol>
<li>$R_k$ minimizes the Frobenius norm $||R-R_k||_F^2$, and</li>
<li>$R_k$ can be constructed by retaining only the largest $k$ singular values in the SVD formulation $R_k=U \cdot \Sigma_k \cdot V^T$.</li>
</ol>

<p>We can further absorb the diagonal matrix $\Sigma_k$ into $U$ and $V$ and express the factorization as a simple dot product between the feature matrices for users and movies.</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $R_{k(m \times n)} = U_{(m \times k)} \cdot V_{(k \times n)}^T$</p>

<p>where, the parentheses indicate matrix size.<br />
$m$: number of users ($m = 138493$)<br />
$n$: number of movies ($n = 27278$)<br />
$k$: rank hyperparameter that we impose (typically $k=10$).<br />
$U$: user feature matrix<br />
$V$: movies feature matrix</p>

<p><img src="/Drawing.png" alt="Drawing" width="1000" /></p>

<p>Hence, we formulate the problem as an <strong>optimization problem</strong> and search for $U$ and $V$ by minimizing the following loss function $L$.</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $argmin_{\ U,V}\ L = ||R-U\cdot V^T||_F^2$.</p>

<blockquote>
<p><em>An important point to make is the Frobenius norm is only a partial summation computed over the entries in $R$ where a rating is provided. In the tensorflow implementation, we don&rsquo;t compute the complete matrix multiplication $U \cdot V^T$ but only the dot products of $u_i \cdot v_j^T$ where a rating $R_{ij}$ is provided.</em></p>
</blockquote>

<p>The optimization procedure searches for the values of all entries in $U$ and $V$. There are $(m+n) \times k$ many tunable variables.</p>

<p>The hyperparameter $k$ is to be chosen carefully by cross-validation. A small $k$ would not be enough to explain the pattern in the data adequately (<em>underfitting</em>), and too large a $k$ value would result in a model fitting on the random noise over the pattern (<em>overfitting</em>).</p>

<blockquote>
<p><em>It&rsquo;s a worth making a brief interpretation of the feature matrices $U$ and $V$. In the $k$-rank approximation scheme, each rating $R_{ij}$ is expressed by the dot product $u_i \cdot v_j^T$ &mdash;$i^{th}$ row of $U$ and $j^{th}$ row of $V$. The goal of our optimization routine is for the model to learn a</em> <strong><em>latent feature representation</em></strong> <em>(or alternatively an</em> <strong><em>embedding vector</em></strong>) <em>for each user $u_i$, and movie $v_j$.  By the word latent, it&rsquo;s implied that the features are not explicitly defined by us nor they&rsquo;re clearly  interpretable. Each entry in the user and movie embedding vectors $u_i$ and $v_j$ corresponds to an abstract feature. These features can, for instance, be the genre of the movie, or how action-filled or dramatic, entertaining or romantic the movie is or anything that would help characterize how the users rate movies.</em><br />
<br />
<em>Hence, the dot product representation of the ratings $r_{ij} = u_i \cdot v_j^T$ points to a linear combination of how much that feature is contained in the movie and how much that feature is favored by the user.</em>
<br />
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $r_{ij} = \sum_{p=1}^k u_{ip}v_{jp}$</p>
</blockquote>

<h5 id="n-b-explicitly-defining-biases-is-not-necessary">N.B. Explicitly  Defining Biases is not Necessary</h5>

<p>We abstain from imposing <strong>biases</strong> by enforcing an extra component in $U$ and $V$ set to constant 1, since the embeddings are free to learn biases if necessary.</p>

<p>Since no particular bounds are imposed on the entries in the embedding vectors $u_{i}$ and $v_{j}$. The model is free to learn positive or negative real numbers.</p>

<hr />

<h2 id="3-lab41-movie-ratings-data">3. Lab41 movie ratings data</h2>

<ul>
<li>ratings were given at intervals of 0.5: {0.0, 0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0}</li>
</ul>

<hr />

<h2 id="4-challenges-in-developing-the-model-on-tensorflow">4. Challenges in Developing the Model on <code>tensorflow</code></h2>

<ul>
<li>A particular challenge in implementing a Matrix Factorization algorithm on <code>tensorflow</code> is that we can&rsquo;t naively pass <code>None</code> for the <code>shape</code> argument while declaring the input data tensors <code>R</code> and <code>R\_indices</code> as in <code>R = tf.placeholder(..., shape=(None))</code>. Since every rating <code>R[i,j]</code> is a function of only $2*k$ tunable variables. The <code>shape</code> parameter corresponds to how many ratings will make up a single batch in the SGD routine. To make the SGD work, I had to fix the <code>shape</code> of the <code>tf.placeholder</code> <code>R</code> and <code>R\_indices</code>  to <code>shape=(BATCH_SIZE, k)</code> instead of <code>shape=(None, k)</code>. This a small price to pay that allows me to use GPU computation and also backprop with symbolic differentiation, which gave me the flexibility to experiment in trying additional non-linear terms in the loss function without having the worry about the partial differentials with respect to the tunable variables.<br /></li>
</ul>

<pre><code class="language-python">R = tf.placeholder(dtype=tf.float32, shape=(BATCH_SIZE,))
R_indices = tf.placeholder(dtype=tf.int32, shape=(BATCH_SIZE,2))
u_mean = tf.placeholder(dtype=tf.float32, shape=(BATCH_SIZE,1)) 
v_mean = tf.placeholder(dtype=tf.float32, shape=(BATCH_SIZE,1)) 
</code></pre>

<ul>
<li>At each iteration of the SGD algorithm a mini-batch of rating data $R_{ij}$ and the corresponding user and movie index pair $(i,j)$, will be fed into the computational graph. Since each $R_{ij}$ is represented as the dot product $u_i \cdot v_j^T$ by our model, this will require us to create a stack of the corresponding embedding vectors, a <code>U_stack</code> and a <code>V_stack</code> where <code>U_stack.getshape() == (BATCH_SIZE,k)</code> and <code>V_stack.getshape() == (BATCH_SIZE,k)</code>.</li>
</ul>

<p>The implementation on <code>tensorflow</code> is a little trickier than in <code>numpy</code>, and it&rsquo;s implemented in the following <code>get_stacked_UV</code> module:</p>

<pre><code class="language-python">def get_stacked_UV(R_indices, R, U, V, k, BATCH_SIZE):
    u_idx = R_indices[:,0]
    v_idx = R_indices[:,1]
    rows_U = tf.transpose(np.ones((k,1), dtype=np.int32)*u_idx)
    rows_V = tf.transpose(np.ones((k,1), dtype=np.int32)*v_idx)
    cols = np.arange(k, dtype=np.int32).reshape((1,-1))
    cols = tf.tile(cols, [BATCH_SIZE,1])

    indices_U = tf.stack([rows_U, cols], -1)
    indices_V = tf.stack([rows_V, cols], -1)

    stacked_U = tf.gather_nd(U, indices_U)
    stacked_V = tf.gather_nd(V, indices_V)
    
    return stacked_U, stacked_V
</code></pre>

<hr />

<h2 id="5-practical-methodology">5. Practical Methodology</h2>

<ul>
<li>Shuffling the data before splitting it into train, CV and test sets was crucial.</li>
</ul>

<h5 id="splitting-the-input-data">Splitting the Input Data:</h5>

<ul>
<li>Training Data takes up 64% of the input data,</li>
<li>CV Data 16% and</li>
<li>Test Data 20%.</li>
</ul>

<h2 id="7-linear-vs-non-linear-features">7. Linear vs Non-linear features</h2>

<ul>
<li><code>R_pred = tf.sigmoid(R_pred) * 5</code> dropped the <code>MAE_test</code> approximately from <code>.64</code> to <code>.62</code>. Firstly, I can&rsquo;t explain why sigmoid works better&ndash;although only by a tiny bit.</li>
<li>However, adding squared dot product term along with the linear dot product, didn&rsquo;t produce any tangible improvement.</li>
</ul>

<pre><code class="language-python">u_cdot_v_square = tf.square(tf.multiply(sliced_U, sliced_V)) 
nl = tf.reduce_sum(u_cdot_v_square, axis=1)
R_pred = R_pred + alpha*nl
R_pred = tf.sigmoid(R_pred) * 5
</code></pre>

<h2 id="8-linear-features">8. Linear Features:</h2>

<p>Matrix factorization is based on a low-rank singular value decomposition (SVD).</p>

<p>$$R=U \cdot V^{T}$$</p>

<p>An individual rating of user $i$ on movie $j$ is given by</p>

<p>$$r_{ij} = u_{i} \cdot v_{j}^T$$</p>

<p>Here, each user feature vector $u_i$ and movie feature vector $v_j$ is of length $k$. and the classical matrix factorization multiplies $p^{th}$ feature of $u_{i}$ with  $p^{th}$ feature of $v_{j}$. Here, one can assume the feature $p$ corresponds to how much of a specific genre is present in movie $j$ and how much a user $i$ likes that specific genre. When the rating $y_{ij}$ is modeled by a dot product between $u_i$ and $v_j$.</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;
<strong>=&gt; Linear Model: MAE (CV) = 0.6237</strong></p>

<h2 id="9-nonlinear-cross-features">9. Nonlinear Cross-Features:</h2>

<p>The rating prediction with cross-features</p>

<p>$$r_{ij} = u_{i} \cdot v_{j}^{T} + \sum_{p=0}^{k}\sum_{q=0}^{k} w_{pq} (u_{ip} \cdot v_{jq}^{T})$$</p>

<p>Here, we&rsquo;re multiplying $p^{th}$ feature of user $i$ with $q^{th}$ feature of movie $j$. This allows the model to learn cross interactions as, say, if a user likes the actor Tom Cruise (the $p^{th}$ feature&ndash;high  value for $u_{ip}$), and she doesn&rsquo;t like dark and suspenseful thrillers ($q^{th}$ feature&ndash;low value for $u_{iq}$), however, she likes the movie Eyes Wide Shut (even though it has a high value for $v_{jq}$), because an underlying reason that makes her not like dark suspenseful movies perhaps disappears if Tom Cruise is in the movie. For a model to capture such a pattern, it has to allow some sort of <strong>nonlinear interactions</strong> between feature $p$ and feature $q$.</p>

<p>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; <strong>=&gt; Non-linear Model: MAE (CV) = 0.6150</strong></p>

<pre><code>R_pred = np.dot(U,V) + alpha1*(xft) + alpha2*(uv_sq)
</code></pre>

<p>The runtime for one epoch went from $31$ sec for linear model to $60$ sec when considering all 3 types of nonlinear feature crossings ($u_{ip}$ ~ $v_{jq}$), ($u_{ip}$ ~ $u_{iq}$), ($v_{jp}$ ~ $v_{jq}$)</p>

<h2 id="10-improvements-on-the-algorithm">10. Improvements on the Algorithm</h2>

<ol>
<li>The regularization term needs to take into account the average rating for each user $u_i$, $\mu_{u_i}$.</li>
</ol>

      </div>
    </article>
    <aside class="ph3 mt2 mt6-ns">
      







  <div class="bg-light-gray pa3">
    <ul>
      <li class="list b mb3">
        1 More Posts
      </li>
      
        <li class="list f5 w-100 hover-bg-white nl1">
          
          <a href="/posts/semi-supervised-learning/" class="link ph2 pv2 db black">
            Semi-supervised Learning
          </a>
        </li>
      
        <li class="list f5 w-100 hover-bg-white nl1">
          
          <a href="/posts/collaborative-filtering-tensorflow/" class="link ph2 pv2 db black o-50">
            Collaborative Filtering with tensorflow
          </a>
        </li>
      
    </ul>
  </div>


    </aside>
  </div>

    </main>
    <footer class="bg-near-black bottom-0 w-100 pa3" role="contentinfo">
  <div class="flex justify-between">
  <a class="f4 fw4 hover-white no-underline white-70 dn dib-ns pv2 ph3" href="https://ozkansafak.github.io/" >
    &copy; 2017 Theory of Machines
  </a>
  








  </div>
</footer>

    <script src="https://ozkansafak.github.io/dist/app.bundle.js" async></script>

  </body>
</html>
