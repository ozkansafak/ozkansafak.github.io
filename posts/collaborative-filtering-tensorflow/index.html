<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en-us" lang="en-us">
<head>
     <script type="text/x-mathjax-config">MathJax.Hub.Config({ tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]} }); 
     </script> 
     <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"> 
     </script> 

  <link href="http://gmpg.org/xfn/11" rel="profile">
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="generator" content="Hugo 0.31" />

  
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <title>A Collaborative Filtering Model on tensorflow with Nonlinear Cross Features &middot; Theory of Machines</title>

  
  <link rel="stylesheet" href="https://ozkansafak.github.io/css/print.css" media="print">
  <link rel="stylesheet" href="https://ozkansafak.github.io/css/poole.css">
  <link rel="stylesheet" href="https://ozkansafak.github.io/css/syntax.css">
  <link rel="stylesheet" href="https://ozkansafak.github.io/css/hyde.css">
  <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Abril+Fatface|PT+Sans:400,400i,700">

  
  <link rel="apple-touch-icon-precomposed" sizes="144x144" href="/apple-touch-icon-144-precomposed.png">
  <link rel="shortcut icon" href="/favicon.png">

  
  <link href="" rel="alternate" type="application/rss+xml" title="Theory of Machines" />
</head>

  <body class=" ">
       <script type="text/x-mathjax-config">MathJax.Hub.Config({ tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]} }); 
     </script> 
     <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-MML-AM_CHTML"> 
     </script> 

<div class="sidebar">
  <div class="container sidebar-sticky">
    <div class="sidebar-about">
      <a href="https://ozkansafak.github.io/"><h1>Theory of Machines</h1></a>
      <p class="lead">
       Safak Ozkan's Blog 
      </p>
    </div>

    <ul class="sidebar-nav">
      <li><a href="https://ozkansafak.github.io/">Home</a> </li>
      
    </ul>

    <p>&copy; 2017. All rights reserved. </p>
  </div>
</div>

    <div class="content container">
    <div class="post">
  <h1>A Collaborative Filtering Model on tensorflow with Nonlinear Cross Features</h1>
  <span class="post-date">Sat, Nov 18, 2017</span>
  

<hr />

<h2 id="1-problem-description">1. Problem Description</h2>

<p>We are given a rating matrix $R$ where only a small fraction of the entries $R_{ij}$ are provided; otherwise the rest is missing. The task is to predict those missing entries. As in most Machine Learning problems the assumption here is that there&rsquo;s an underlying stationary pattern as to how users rate the movies.</p>

<p>By the nature of the problem, $R$ is a sparse matrix, where the sparsity comes not from zero entries but from empty records. Therefor, we represent the training data in 3 columns: $i$: user ID , $j$: movie ID and $R_{ij}$: the rating (see Table 1).</p>

<p><font size="+1"><strong><p align="center">Table 1. Ratings data ml-20m sparse format</p></strong></font></p>

<table>
<thead>
<tr>
<th align="center">$i$: user ID</th>
<th align="center">$j$: movie ID</th>
<th align="center">$R_{ij}$: the rating</th>
</tr>
</thead>

<tbody>
<tr>
<td align="center">0</td>
<td align="center">14</td>
<td align="center">3.5</td>
</tr>

<tr>
<td align="center">0</td>
<td align="center">7305</td>
<td align="center">4.0</td>
</tr>

<tr>
<td align="center">0</td>
<td align="center">16336</td>
<td align="center">3.5</td>
</tr>

<tr>
<td align="center">.</td>
<td align="center">.</td>
<td align="center">.</td>
</tr>

<tr>
<td align="center">.</td>
<td align="center">.</td>
<td align="center">.</td>
</tr>

<tr>
<td align="center">1</td>
<td align="center">52</td>
<td align="center">4.0</td>
</tr>

<tr>
<td align="center">1</td>
<td align="center">986</td>
<td align="center">4.0</td>
</tr>

<tr>
<td align="center">1</td>
<td align="center">1455</td>
<td align="center">3.5</td>
</tr>

<tr>
<td align="center">1</td>
<td align="center">1705</td>
<td align="center">5.0</td>
</tr>

<tr>
<td align="center">1</td>
<td align="center">5598</td>
<td align="center">4.0</td>
</tr>

<tr>
<td align="center">.</td>
<td align="center">.</td>
<td align="center">.</td>
</tr>

<tr>
<td align="center">.</td>
<td align="center">.</td>
<td align="center">.</td>
</tr>

<tr>
<td align="center">138493</td>
<td align="center">27278</td>
<td align="center">5.0</td>
</tr>
</tbody>
</table>

<hr />

<h2 id="2-collaborative-filtering-model">2. Collaborative Filtering Model</h2>

<p>The terms <em>Collaborative Filtering</em>, <em>Matrix Factorization</em> and <em>Low-Rank Matrix Factorization</em> all refer to the same recommender system model. In essence, this model is based on the assumption that users who liked the same movies are likely to feel similarly towards other movies. The term <em>collaborative</em> refers to the observation that when a large set of users are involved in rating the movies, these users are effectively collaborating to get better movie ratings for everyone because every new rating will help the algorithm learn better features for the <em>users-movies</em> system. Later, these features are used by the model to make better rating predictions for everyone.</p>

<p>The Collaborative Filtering Model can also be described as reconstructing a <strong>low rank approximation</strong> of matrix $R$ via its <strong>Singular Value Decomposition</strong> $R = U \cdot \Sigma \cdot V^T$. The low-rank reconstruction is achieved by only retaining the largest $k$ singular values, $R_k=U \cdot \Sigma_k \cdot V^T$.</p>

<p><strong>Eckart-Young Theorem</strong> states that if $R_k$ is the best rank-$k$ approximation of $R$, then it&rsquo;s necessary that:</p>

<p>&emsp;&emsp;&emsp;   1. $R_k$ minimizes the Frobenius norm $||R-R_k||_F^2$ and<br />
&emsp;&emsp;&emsp;   2. $R_k$ can be constructed by retaining only the largest $k$ singular values in $\Sigma_k$ of the SVD formulation.</p>

<p>We can further absorb the diagonal matrix $\Sigma_k$ into $U$ and $V$ and express the factorization as a simple dot product between the feature matrices for users and movies.</p>

<p align="center">$\hat{R}_{k(m \times n)} = U_{(m \times k)} \cdot V_{(k \times n)}^T$</p>

<p>where, the parentheses indicate matrix size.<br />
$m$: number of users ($m = 138493$)<br />
$n$: number of movies ($n = 27278$)<br />
$k$: rank hyperparameter that we impose (typically $k=10$).<br />
$U$: user feature matrix<br />
$V$: movies feature matrix</p>

<p>Hence, we can formulate the problem as an <strong>optimization problem</strong> and search for all the entries in $U$ and $V$ by minimizing the following loss function $L$ via SGD.</p>

<p>$$argmin_{\ U,V}\ L = ||R - \hat{R}||_F^2$$</p>

<p>It&rsquo;s important to note that the Frobenius norm is computed only as a <strong>partial summation</strong> over the entries in $\hat{R}$ where a rating is provided&mdash;or equivalently over the list of ratings as shown in Table 1. The optimization procedure searches for the values of all entries in $U$ and $V$. There are $(m+n) \times k$ many tunable variables.</p>

<p>The hyperparameter $k$ is to be chosen carefully by cross-validation. Too small a $k$ value would not be enough to explain the pattern in the data adequately (<em>underfitting</em>), and too large a $k$ value would result in a model fitting on the random noise over the pattern (<em>overfitting</em>).</p>

<p>It&rsquo;s worth making a brief interpretation of the feature matrices $U$ and $V$. In the $k$-rank approximation scheme, each rating $R_{ij}$ is expressed as the dot product $U_i^\ \cdot V_j^T$ as shown in Figure 1. The goal of our optimization routine is for the model to learn a <strong>latent feature vector</strong> (or alternatively an <strong>embedding vector</strong>) for each user and movie.  The term latent implies that the features are not explicitly defined as a part of the model nor they can be interpreted definitively once the embeddings are learned. Each entry in $U_i$ and $V_j$ corresponds to the weight coefficient of an abstract feature. These features can specify the genre of the movie or how much action or drama contained in the movie or any other distinguishing quality that would help characterize how the users rate movies. Hence, the dot product representation of the ratings $R_{ij} = U_i^\ \cdot V_j^T$ expresses a <strong>linear combination</strong> of</p>

<p>&emsp;&emsp;&emsp; 1. how much that feature is contained in the movie-$j$, and<br />
&emsp;&emsp;&emsp; 2. how much that feature is favored by the user-$i$.</p>

<p><img src="/Drawing.png" alt="Drawing" width="1000" />
<font size="+1"><p align="center"><b>Figure 1. A conceptual sketch of the Ratings data matrix $R$ decomposed into its factors: user feature matrix, $U$, and movie feature matrix, $V$. Dots in the figure &ldquo;$\cdot$&rdquo; illustrate given values; and question marks &ldquo;$?$&rdquo; the missing values. Each entry $R_{ij}$ is expressed as a dot product of the user and movie embedding vectors $U_i$ and $V_j$, respectively.</b></p></font></p>

<hr />

<h2 id="3-movielens-20m-dataset">3. MovieLens 20M dataset</h2>

<ul>
<li><a href="https://grouplens.org/datasets/movielens/20m/">MovieLens dataset</a> data set consists of 20,000,263 ratings from 138,493 users on 27,278 movies. Ratings are provided for only $0.5\%$ of all possible entries in $R$.</li>
<li>All ratings are given at intervals of 0.5:<br />
{0.5, 1.0, 1.5, 2.0, 2.5, 3.0, 3.5, 4.0, 4.5, 5.0}</li>
<li>Since the input data was ordered according to user IDs, it was crucial to shuffle the data before splitting it into training, CV and test sets.</li>
<li>The Input Data is split accordingly:

<ul>
<li>$64\%$ &ndash; training data,</li>
<li>$16\%$ &ndash; cross validation data,</li>
<li>$20\%$ &ndash; test data.</li>
</ul></li>
<li>We abstain from explicitly imposing a <strong>bias term</strong> by enforcing an extra component that&rsquo;s equal to constant $1$ in $U$ and $V$. In the matrix factorization scheme, the embeddings are free to learn biases if necessary.</li>
<li>Since no particular bounds are imposed on the entries in the embedding vectors $U_{i}$ and $V_{j}$. The model is free to learn positive or negative real numbers.<br /></li>
</ul>

<p><div style="width:700 px">
    <div style="float:left; width:360">
        <img src="/fig1.png" alt="fig1" width="360" />
    </div>
    <div style="float:left; width:300">
        <img src="/fig2.png" alt="fig2" width="360" />
    </div>
    <div style="float:left; width:300">
        <img src="/fig3.png" alt="fig3" width="360" />
    </div>
    <div style="float:left; width:300">
        <img src="/fig4.png" alt="fig4" width="360" />
    </div>
    <div style="clear:both"></div>
</div>
<br>
<font size="+1"><b><p align="center">Figure 2. Histogram of (<em>a</em>) all ratings in ml-20m data (<em>b</em>) mean of ratings per user (<em>c</em>) mean of ratings per movie, and (<em>d</em>)
    number of ratings provided by users. Minimum number of ratings provided by a user is 20, and maximum is 9254 ratings.
</p></b></font></p>

<hr />

<h2 id="4-linear-nonlinear-and-cross-features">4. Linear, Nonlinear and Cross Features</h2>

<h5 id="linear-terms">Linear Terms:</h5>

<ul>
<li><p>Standard collaborative filtering model consists of only the linear term, where each rating is expressed as a dot product of the feature vectors:
$$(R_{lin})_{ij} = U_{i}^\ \cdot V_{j}^T$$
In this dot product, $p^{th}$ feature coefficient of $U_{i}$ is multiplied with the corresponding $p^{th}$ coefficient of $V_{j}$. The contributions from each feature are added up into a total summation.</p></li>

<li><p>Subsequently, adding a sigmoid filter $R_{ij} = 5 \cdot \sigma(U_i^\ \cdot V_j^T)$, where $\sigma$ is the sigmoid function, lowered the MAE Rate from approximately <code>.64</code> to <code>.62</code>. The reason for this is that in the absence of sigmoid activation, some predictions fall outside the range $[0, 5]$. Sigmoid activation squashes the predictions to the correct range and hence closer to their actual values.<br />
The Mean Absolute Error (MAE) on the cross validation set for the pure linear model is:<br />
$$\hat{R} = R_{lin}$$
$$=&gt; Linear\ Model: MAE (CV) = 0.624$$</p></li>
</ul>

<h5 id="nonlinear-terms">Nonlinear Terms:</h5>

<ul>
<li><p>I experimented with adding a 2<sup>nd</sup> order term to the rating model:<br />
$$(R_{nl})_{ij} = \sum_{p=1}^k \Big[U_{ip} V_{jp}\Big]^2$$</p></li>

<li><p>However, the quadratic terms didn&rsquo;t result in any discernible reduction in the final error rate and it was not used in the final model.</p></li>
</ul>

<h5 id="cross-feature-terms">Cross Feature Terms:</h5>

<ul>
<li><p>Cross feature terms will introduce the following 2<sup>nd</sup> order nonlinearity to the rating model:<br />
$$(R_{xft})_{ij} = \sum_{p=1}^{k}\sum_{q=1}^{k} \Big[(X_{UV})_{pq}\ U_{ip} V_{jq}\Big]$$
$$+\sum_{p=1}^{k}\sum_{q=p+1}^{k} \Big[(X_{UU})_{pq}\ U_{ip} U_{jq}+ (X_{VV})_{pq}\ V_{ip}  V_{jq}\Big]$$
where<br />
$X_{UV}$, $X_{UU}$, $X_{VV}$ are cross feature coefficient tensors of size $k \times k$ to be learned by the SGD optimization algorithm.<br />
<br />
For instance, in the second term of the above equation, feature-$p$ of $U_i$ gets multiplied by feature-$q$ of $V_j$ and the contribution to the rating $R_{ij}$ is controlled by the cross feature coefficient $(X_{UV})_{pq}$. The same feature crossings are also performed within user vectors in 3rd term and movie vectors in the 4th term.<br />
<br />
This allows the model to learn cross interactions. The interpretation of these terms could be made as follows: for example for the $U_i$-$V_j$ term, if a user likes the actor Tom Cruise (a large value for $U_{ip}$), but she doesn&rsquo;t like dark and suspenseful movies (a small value for $U_{iq}$), however, she likes the movie Eyes Wide Shut (even though it has a high value for $V_{jq}$), perhaps because an underlying reason that makes her not like dark suspenseful movies perhaps vanishes if Tom Cruise is in the movie. For a model to capture such a pattern, it has to allow some sort of <strong>nonlinear cross feature interactions</strong> between features $p$ and $q$. The gain in the addition of cross feature terms over the linear terms on the MAE Rate is a mere $1.5\%$:
$$\hat{R} = R_{lin} + R_{xft}$$
$$=&gt; Nonlinear\ Cross\ Feature\ Model: MAE (CV) = 0.615$$</p></li>

<li><p>My intuition is that cross features must play a crucial role in explaining the rating patterns in the user population. Yet, we&rsquo;re only adding $3 k^2$ more tunable variables that are aimed to capture this nonlinear pattern for the whole population of users and movies. On the other hand, the feature matrices $U$ and $V$ carry $12800168 \cdot 2k$ tunable variables for the whole training set. For a typical value of $k=10$, the linear feature terms have approximately 850,000 more tunable variables than the cross feature terms.<br />
<br />
A better model that&rsquo;s aimed to capture the cross feature interactions for each user independently could incorporate a new set of $X_{UU}$ tensors for each user separately. However this would introduce $mk^<sup>2</sup>&frasl;<sub>2</sub>$ ($\approx$ 6,924,650) tunable parameters. However, this would quickly become impractical, however, would be a curious undertaking on <code>tensorflow</code>.</p></li>

<li><p>The computational price paid for a mere $1.5\%$ increase in MAE Rate is that the runtime increased from $30\ sec/epoch$ for pure linear model to $60\ sec/epoch$ when incorporating all 3 types of nonlinear feature crossings:
<p align="center">($U_{ip}$ ~ $V_{jq}$),&nbsp; ($U_{ip}$ ~ $U_{iq}$),&nbsp; ($V_{jp}$ ~ $V_{jq}$)</p></p></li>
</ul>

<hr />

<h2 id="5-regularization">5. Regularization</h2>

<p>An $L_2$ regularization term naively applied on the feature vectors $U$ and $V$ would penalize all nonzero components. This would encourage the coefficients in $U$ and $V$ to be small.  However, we would rather have the coefficients in $U$ and $V$ regress towards the mean rating of the corresponding user (or alternatively the mean rating of the corresponding movie) instead of zero. And for the non-linear cross feature terms I kept the regularization term naive to regress towards the value zero.</p>

<p>In essence, we are imposing a penalty for any behavior that diverges from the average pattern.  In this spirit, I formulated the $L_2$ regularization term as follows:</p>

<p>$$\Omega_{linear} = \sum_{i=1}^m\ \Big(U_i - \mu_{U_i} \Big)^2 +\sum_{j=1}^n\ \Big(V_j - \mu_{V_j}\ \Big)^2$$
$$\Omega_{xft} = \sum_{p=1}^k \sum_{q=1}^k\ (X_{UV})_{pq} + \sum_{p=1}^k \sum_{q=1}^k\ (X_{UU})_{pq} + \sum_{p=1}^k \sum_{q=1}^k\ (X_{VV})_{pq}$$</p>

<p>where,<br />
$\Omega_{linear}$: regularization term for linear features<br />
$\Omega_{xft}$: regularization term for cross features<br />
$\mu_{U_i}$: mean rating for user-$i$<br />
$\mu_{V_j}$: mean rating for movie-$j$<br />
$UV_{ij}$: cross feature term between user-$i$ and movie-$j$<br />
$UU_{ij}$: cross feature term between user-$i$ and user-$j$<br />
$VV_{ij}$: cross feature term between movie-$i$ and movie-$j$</p>

<pre><code class="language-python">reg = (tf.reduce_sum((stacked_U - stacked_u_mean)**2) + 
       tf.reduce_sum((stacked_V - stacked_v_mean)**2) + 
       tf.reduce_sum((UV_xft**2)) + 
       tf.reduce_sum((UU_xft**2)) + 
       tf.reduce_sum((VV_xft**2))) / (BATCH_SIZE*k)
</code></pre>

<p>However, regularization didn&rsquo;t improve the MAE Rate because of the over abundance of data which is already the best implicit regularizer than any other explicitly devised regularization term.</p>

<hr />

<h2 id="6-practical-methodology-and-developing-the-model-on-tensorflow">6. Practical Methodology and Developing the Model on <code>tensorflow</code></h2>

<ul>
<li>A particular challenge in implementing a Matrix Factorization algorithm on <code>tensorflow</code> is that we can&rsquo;t naively pass <code>None</code> for the <code>shape</code> argument while declaring the input data tensors <code>R</code> and <code>R_indices</code> as in <code>R = tf.placeholder(..., shape=(None))</code>.  The <code>shape</code> parameter corresponds to the number of ratings a single batch in the SGD step contains. To make the SGD work, I had to fix the <code>shape</code> of the <code>tf.placeholder</code> variables <code>R</code> and <code>R_indices</code>  to <code>shape=(BATCH_SIZE, k)</code> instead of <code>shape=(None, k)</code>.  This is a small price to pay that allows me to use <code>tensorflow</code> which provides me GPU computation and also backprop with symbolic differentiation. This gave me the flexibility to experiment with additional nonlinear terms in loss function without having the worry about the partial differentials with respect to the tunable variables.</li>
</ul>

<pre><code class="language-python">R = tf.placeholder(dtype=tf.float32, shape=(BATCH_SIZE,))
R_indices = tf.placeholder(dtype=tf.int32, shape=(BATCH_SIZE,2))
u_mean = tf.placeholder(dtype=tf.float32, shape=(BATCH_SIZE,1)) 
v_mean = tf.placeholder(dtype=tf.float32, shape=(BATCH_SIZE,1)) 
</code></pre>

<ul>
<li>At each SGD step a mini-batch of rating data $R_{ij}$ and the corresponding user-movie index pairs $(i,j)$ are fed into the computational graph. Since each $R_{ij}$ is represented as the dot product $U_i^\ \cdot V_j^T$, we have to stack the corresponding embedding vectors into 2-D tensors <code>U_stack</code> and <code>V_stack</code> where both <code>U_stack.getshape()</code> and <code>V_stack.getshape()</code> equal to <code>(BATCH_SIZE,k)</code>.<br />
The implementation of stacking tensors  on<code>tensorflow</code> is a little trickier than in <code>numpy</code>. It&rsquo;s done like this:<br /></li>
</ul>

<pre><code class="language-python">def get_stacked_UV(R_indices, R, U, V, k, BATCH_SIZE):
    u_idx = R_indices[:,0]
    v_idx = R_indices[:,1]
    rows_U = tf.transpose(np.ones((k,1), dtype=np.int32)*u_idx)
    rows_V = tf.transpose(np.ones((k,1), dtype=np.int32)*v_idx)
    cols = np.arange(k, dtype=np.int32).reshape((1,-1))
    cols = tf.tile(cols, [BATCH_SIZE,1])

    indices_U = tf.stack([rows_U, cols], -1)
    indices_V = tf.stack([rows_V, cols], -1)

    stacked_U = tf.gather_nd(U, indices_U)
    stacked_V = tf.gather_nd(V, indices_V)
    
    return stacked_U, stacked_V
</code></pre>

<h5 id="initialization">Initialization:</h5>

<ul>
<li>The feature vectors $U$ and $V$ are initialized by sampling from a Gaussian Distribution with mean $\mu = \sqrt{ 3.5/k}$, and standard deviation $\sigma = 0.2$. For the cross feature vectors $X_{UV}$, $X_{UV}$ and $X_{VV}$, the mean was chosen by practical experiments to be $\mu=-1/k$ and standard deviation $\sigma = 0.2$ as in the linear feature vectors.</li>
</ul>

<hr />

<h2 id="references">References:</h2>

<ol>
<li>Learning From Data, Yaser S. Abu-Mostafa, Malik Magdon-Ismail, Hsuan-Tien Lin, 2012.</li>
<li>Machine Learning, Andrew Ng, Coursera online class, 2011.</li>
<li>Deep Learning, Ian Goodfellow, Yoshua Bengio, Aaron Courville, 2016.</li>
</ol>

</div>


    </div>

    
  </body>
</html>